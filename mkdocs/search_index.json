{
    "docs": [
        {
            "location": "/", 
            "text": "pyfolio\n\n\n\n\n\n\npyfolio is a Python library for performance and risk analysis of\nfinancial portfolios developed by\n\nQuantopian Inc\n. It works well with the\n\nZipline\n open source backtesting library.\n\n\nAt the core of pyfolio is a so-called tear sheet that consists of\nvarious individual plots that provide a comprehensive image of the\nperformance of a trading algorithm. Here is an example of a tear sheet of a the Zipline algo included as one of our example notebooks:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also \nslides of a recent talk about pyfolio.\n\n\nInstallation\n\n\nTo install \npyfolio\n via \npip\n issue the following command:\n\n\npip install pyfolio\n\n\n\n\nFor development, clone the git repo and run \npython setup.py develop\n\nand edit the library files directly. Make sure to reload or restart\nthe IPython kernel when you make changes.\n\n\npyfolio\n has the following dependencies:\n - numpy\n - scipy\n - pandas\n - matplotlib\n - \nseaborn\n\n - \npymc3\n (optional)\n - \nzipline\n (optional; requires master, \nnot\n 0.7.0)\n\n\nQuestions?\n\n\nIf you find a bug, feel free to open an issue on our github tracker.\n\n\nYou can also join our \nmailing list\n.\n\n\nContribute\n\n\nIf you want to contribute, a great place to start would be the \nhelp-wanted issues\n.\n\n\nCredits\n\n\n\n\nGus Gordon (gus@quantopian.com)\n\n\nJustin Lent (justin@quantopian.com)\n\n\nSepideh Sadeghi (sp.sadeghi@gmail.com)\n\n\nThomas Wiecki (thomas@quantopian.com)\n\n\nJessica Stauth (jstauth@quantopian.com)\n\n\nKaren Rubin (karen@quantopian.com)\n\n\nDavid Edwards (dedwards@quantopian.com)\n\n\nAndrew Campbell (andrew@quantopian.com)\n\n\n\n\nFor a full list of contributors, see https://github.com/quantopian/pyfolio/graphs/contributors.", 
            "title": "Overview"
        }, 
        {
            "location": "/#pyfolio", 
            "text": "pyfolio is a Python library for performance and risk analysis of\nfinancial portfolios developed by Quantopian Inc . It works well with the Zipline  open source backtesting library.  At the core of pyfolio is a so-called tear sheet that consists of\nvarious individual plots that provide a comprehensive image of the\nperformance of a trading algorithm. Here is an example of a tear sheet of a the Zipline algo included as one of our example notebooks:         See also  slides of a recent talk about pyfolio.", 
            "title": "pyfolio"
        }, 
        {
            "location": "/#installation", 
            "text": "To install  pyfolio  via  pip  issue the following command:  pip install pyfolio  For development, clone the git repo and run  python setup.py develop \nand edit the library files directly. Make sure to reload or restart\nthe IPython kernel when you make changes.  pyfolio  has the following dependencies:\n - numpy\n - scipy\n - pandas\n - matplotlib\n -  seaborn \n -  pymc3  (optional)\n -  zipline  (optional; requires master,  not  0.7.0)", 
            "title": "Installation"
        }, 
        {
            "location": "/#questions", 
            "text": "If you find a bug, feel free to open an issue on our github tracker.  You can also join our  mailing list .", 
            "title": "Questions?"
        }, 
        {
            "location": "/#contribute", 
            "text": "If you want to contribute, a great place to start would be the  help-wanted issues .", 
            "title": "Contribute"
        }, 
        {
            "location": "/#credits", 
            "text": "Gus Gordon (gus@quantopian.com)  Justin Lent (justin@quantopian.com)  Sepideh Sadeghi (sp.sadeghi@gmail.com)  Thomas Wiecki (thomas@quantopian.com)  Jessica Stauth (jstauth@quantopian.com)  Karen Rubin (karen@quantopian.com)  David Edwards (dedwards@quantopian.com)  Andrew Campbell (andrew@quantopian.com)   For a full list of contributors, see https://github.com/quantopian/pyfolio/graphs/contributors.", 
            "title": "Credits"
        }, 
        {
            "location": "/whatsnew/", 
            "text": "What's New\n\n\nThese are new features and improvements of note in each release.\n\n\nv0.4.0 (Dec 10, 2015)\n\n\nThis is a major release from 0.3.1 that includes new features and quite a few bug fixes. We recommend that all users upgrade to this new version.\n\n\nNew features\n\n\n\n\nRound-trip analysis \nPR210\n Andrew, Thomas\n\n\nImproved cone to forecast returns that uses a bootstrap instead of linear forecasting \nPR233\n Andrew, Thomas\n\n\nPlot max and median long/short exposures \nPR237\n Andrew\n\n\n\n\nBug fixes\n\n\n\n\nSharpe ratio was calculated incorrectly \nPR219\n Thomas, Justin\n\n\nannual_return() now only computes CAGR in the correct way \nPR234\n Justin\n\n\nCache SPY and Fama-French returns in home-directory instead of install-directory \nPR241\n Joe\n\n\nRemove data files from package \nPR241\n Joe\n\n\nCast factor.name to str \nPR223\n Scotty\n\n\nTest all \ncreate_*_tear_sheet\n functions in all configurations \nPR247\n Thomas\n\n\n\n\nv0.3.1 (Nov 12, 2015)\n\n\nThis is a minor release from 0.3 that includes mostly bugfixes but also some new features. We recommend that all users upgrade to this new version.\n\n\nNew features\n\n\n\n\nAdd Information Ratio \nPR194\n by @MridulS\n\n\nBayesian tear-sheet now accepts 'Fama-French' option to do Bayesian multivariate regression against Fama-French risk factors \nPR200\n by Shane Bussman\n\n\nPlotting of monthly returns \nPR195\n\n\n\n\nBug fixes\n\n\n\n\npos.get_percent_alloc\n was not handling short allocations correctly \nPR201\n\n\nUTC bug with cached Fama-French factors \ncommit\n\n\nSector map was not being passed from \ncreate_returns_tearsheet\n \ncommit\n\n\nNew sector mapping feature was not Python 3 compatible \nPR201\n\n\n\n\nMaintenance\n\n\n\n\nWe now depend on pandas-datareader as the yahoo finance loaders from pandas will be deprecated \nPR181\n by @tswrightsandpointe\n\n\n\n\nContributors\n\n\nBesiders the core developers, we have seen an increase in outside contributions which we greatly appreciate. Specifically, these people contributed to this release:\n\n\n\n\nShane Bussman\n\n\n@MridulS\n\n\n@YihaoLu\n\n\n@jkrauss82\n\n\n@tswrightsandpointe\n\n\n@cgdeboer\n\n\n\n\nv0.3 (Oct 23, 2015)\n\n\nThis is a major release from 0.2 that includes many exciting new features. We recommend that all users upgrade to this new version.\n\n\nNew features\n\n\n\n\nSector exposures: sum positions by sector given a dictionary or series of symbol to sector mappings \nPR166\n\n\nAbility to make cones with multiple shades stdev regions \nPR168\n\n\nSlippage sweep: See how an algorithm performs with various levels of slippage \nPR170\n\n\nStochastic volatility model in Bayesian tear sheet \nPR174\n\n\nAbility to suppress display of position information \nPR177\n\n\n\n\nBug fixes\n\n\n\n\nVarious fixes to make pyfolio pandas 0.17 compatible\n\n\n\n\nv0.2 (Oct 16, 2015)\n\n\nThis is a major release from 0.1 that includes mainly bugfixes and refactorings but also some new features. We recommend that all users upgrade to this new version.\n\n\nNew features\n\n\n\n\nVolatility matched cumulative returns plot \nPR126\n.\n\n\nAllow for different periodicity (annualization factors) in the annual_() methods \nPR164\n.\n\n\nUsers can supply their own interesting periods \nPR163\n.\n\n\nAbility to weight a portfolio of holdings by a metric valued \nPR161\n.\n\n\n\n\nBug fixes\n\n\n\n\nFix drawdown overlaps \nPR150\n.\n\n\nMonthly returns distribution should not stack by year \nPR162\n.\n\n\nFix gross leverage \nPR147", 
            "title": "Releases"
        }, 
        {
            "location": "/whatsnew/#whats-new", 
            "text": "These are new features and improvements of note in each release.", 
            "title": "What's New"
        }, 
        {
            "location": "/whatsnew/#v040-dec-10-2015", 
            "text": "This is a major release from 0.3.1 that includes new features and quite a few bug fixes. We recommend that all users upgrade to this new version.  New features   Round-trip analysis  PR210  Andrew, Thomas  Improved cone to forecast returns that uses a bootstrap instead of linear forecasting  PR233  Andrew, Thomas  Plot max and median long/short exposures  PR237  Andrew   Bug fixes   Sharpe ratio was calculated incorrectly  PR219  Thomas, Justin  annual_return() now only computes CAGR in the correct way  PR234  Justin  Cache SPY and Fama-French returns in home-directory instead of install-directory  PR241  Joe  Remove data files from package  PR241  Joe  Cast factor.name to str  PR223  Scotty  Test all  create_*_tear_sheet  functions in all configurations  PR247  Thomas", 
            "title": "v0.4.0 (Dec 10, 2015)"
        }, 
        {
            "location": "/whatsnew/#v031-nov-12-2015", 
            "text": "This is a minor release from 0.3 that includes mostly bugfixes but also some new features. We recommend that all users upgrade to this new version.  New features   Add Information Ratio  PR194  by @MridulS  Bayesian tear-sheet now accepts 'Fama-French' option to do Bayesian multivariate regression against Fama-French risk factors  PR200  by Shane Bussman  Plotting of monthly returns  PR195   Bug fixes   pos.get_percent_alloc  was not handling short allocations correctly  PR201  UTC bug with cached Fama-French factors  commit  Sector map was not being passed from  create_returns_tearsheet   commit  New sector mapping feature was not Python 3 compatible  PR201   Maintenance   We now depend on pandas-datareader as the yahoo finance loaders from pandas will be deprecated  PR181  by @tswrightsandpointe   Contributors  Besiders the core developers, we have seen an increase in outside contributions which we greatly appreciate. Specifically, these people contributed to this release:   Shane Bussman  @MridulS  @YihaoLu  @jkrauss82  @tswrightsandpointe  @cgdeboer", 
            "title": "v0.3.1 (Nov 12, 2015)"
        }, 
        {
            "location": "/whatsnew/#v03-oct-23-2015", 
            "text": "This is a major release from 0.2 that includes many exciting new features. We recommend that all users upgrade to this new version.  New features   Sector exposures: sum positions by sector given a dictionary or series of symbol to sector mappings  PR166  Ability to make cones with multiple shades stdev regions  PR168  Slippage sweep: See how an algorithm performs with various levels of slippage  PR170  Stochastic volatility model in Bayesian tear sheet  PR174  Ability to suppress display of position information  PR177   Bug fixes   Various fixes to make pyfolio pandas 0.17 compatible", 
            "title": "v0.3 (Oct 23, 2015)"
        }, 
        {
            "location": "/whatsnew/#v02-oct-16-2015", 
            "text": "This is a major release from 0.1 that includes mainly bugfixes and refactorings but also some new features. We recommend that all users upgrade to this new version.  New features   Volatility matched cumulative returns plot  PR126 .  Allow for different periodicity (annualization factors) in the annual_() methods  PR164 .  Users can supply their own interesting periods  PR163 .  Ability to weight a portfolio of holdings by a metric valued  PR161 .   Bug fixes   Fix drawdown overlaps  PR150 .  Monthly returns distribution should not stack by year  PR162 .  Fix gross leverage  PR147", 
            "title": "v0.2 (Oct 16, 2015)"
        }, 
        {
            "location": "/single_stock_example/", 
            "text": "Single stock analysis example in pyfolio\n\n\nHere's a simple example where we produce a set of plots, called a tear sheet, for a stock.\n\n\nImport pyfolio\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\nFetch the daily returns for a stock\n\n\nstock_rets = pf.utils.get_symbol_rets('FB')\n\n\n\n\nCreate a full tear sheet for the single stock\n\n\nThis will show charts about returns and shock events.\n\n\npf.create_returns_tear_sheet(stock_rets)\n\n\n\n\nEntire data start date: 2012-05-21 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nBacktest Months: 38\n                   Backtest\nsortino_ratio          1.65\nomega_ratio            1.18\nmax_drawdown          -0.48\ncalmar_ratio           0.79\nannual_return          0.38\nstability              0.87\nsharpe_ratio           0.85\nannual_volatility      0.44\nalpha                  0.20\nbeta                   0.98\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              47.90  2012-05-21 00:00:00+00:00  2012-09-04 00:00:00+00:00   \n1              22.06  2014-03-10 00:00:00+00:00  2014-04-28 00:00:00+00:00   \n2              17.34  2013-10-18 00:00:00+00:00  2013-11-25 00:00:00+00:00   \n4               9.21  2014-10-28 00:00:00+00:00  2014-11-19 00:00:00+00:00   \n3               9.20  2015-03-24 00:00:00+00:00  2015-05-12 00:00:00+00:00\n\n               recovery date duration  \n0  2013-07-25 00:00:00+00:00      309  \n1  2014-07-24 00:00:00+00:00       99  \n2  2013-12-17 00:00:00+00:00       43  \n4  2014-12-22 00:00:00+00:00       40  \n3  2015-06-23 00:00:00+00:00       66\n\n\n2-sigma returns daily    -0.054\n2-sigma returns weekly   -0.110\ndtype: float64\n\n\n/opt/miniconda/lib/python2.7/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\nCreate a full tear sheet for an equal-weight portfolio of:\n\n\n\n\nLong SPY\n\n\nShort QQQ\n\n\nLong GLD\n\n\nLong TLT\n\n\n\n\nAdditionally, we set the live start date as an example.\n\n\nstock_rets_SPY = pf.utils.get_symbol_rets('SPY')\nstock_rets_QQQ = pf.utils.get_symbol_rets('QQQ')\nstock_rets_GLD = pf.utils.get_symbol_rets('GLD')\nstock_rets_TLT = pf.utils.get_symbol_rets('TLT')\n\n\n\n\nportfolio_4_assets_rets = pf.timeseries.portfolio_returns([stock_rets_SPY, -1 * stock_rets_QQQ, stock_rets_GLD, stock_rets_TLT], \n                                exclude_non_overlapping=True)\n\n\n\n\npf.create_full_tear_sheet(portfolio_4_assets_rets, live_start_date='2013-10-22')\n\n\n\n\nEntire data start date: 2004-11-19 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nOut-of-Sample Months: 21\nBacktest Months: 106\n                   Backtest  Out_of_Sample  All_History\nsortino_ratio          0.96          -0.43         0.78\nomega_ratio            1.12           0.95         1.09\nmax_drawdown          -0.11          -0.08        -0.13\ncalmar_ratio           0.41          -0.20         0.28\nannual_return          0.05          -0.02         0.04\nstability              0.94           0.04         0.83\nsharpe_ratio           0.65          -0.28         0.53\nannual_volatility      0.07           0.06         0.07\nalpha                  0.05           0.00         0.04\nbeta                  -0.04          -0.17        -0.05\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              13.06  2012-11-13 00:00:00+00:00  2013-12-19 00:00:00+00:00   \n2               8.96  2008-03-17 00:00:00+00:00  2008-08-15 00:00:00+00:00   \n1               8.85  2008-12-30 00:00:00+00:00  2009-07-29 00:00:00+00:00   \n4               4.66  2010-10-06 00:00:00+00:00  2011-01-27 00:00:00+00:00   \n3               4.38  2012-01-31 00:00:00+00:00  2012-04-03 00:00:00+00:00\n\n               recovery date duration  \n0                        NaN      NaN  \n2  2008-12-10 00:00:00+00:00      193  \n1  2010-06-07 00:00:00+00:00      375  \n4  2011-04-27 00:00:00+00:00      146  \n3  2012-06-01 00:00:00+00:00       89\n\n\n2-sigma returns daily    -0.008\n2-sigma returns weekly   -0.016\ndtype: float64\n\nStress Events\n                                    mean    min    max\nLehmann                            0.001 -0.016  0.031\nUS downgrade/European Debt Crisis  0.002 -0.018  0.015\nFukushima                          0.001 -0.006  0.007\nEZB IR Event                       0.000 -0.007  0.004\nAug07                              0.000 -0.006  0.005\nSept08                             0.002 -0.016  0.031\n2009Q1                            -0.001 -0.012  0.015\n2009Q2                            -0.000 -0.012  0.020\nFlash Crash                        0.002 -0.009  0.015\nApr14                              0.000 -0.005  0.009\nOct14                             -0.000 -0.007  0.007", 
            "title": "Single stock"
        }, 
        {
            "location": "/single_stock_example/#single-stock-analysis-example-in-pyfolio", 
            "text": "Here's a simple example where we produce a set of plots, called a tear sheet, for a stock.", 
            "title": "Single stock analysis example in pyfolio"
        }, 
        {
            "location": "/single_stock_example/#import-pyfolio", 
            "text": "%matplotlib inline\nimport pyfolio as pf", 
            "title": "Import pyfolio"
        }, 
        {
            "location": "/single_stock_example/#fetch-the-daily-returns-for-a-stock", 
            "text": "stock_rets = pf.utils.get_symbol_rets('FB')", 
            "title": "Fetch the daily returns for a stock"
        }, 
        {
            "location": "/single_stock_example/#create-a-full-tear-sheet-for-the-single-stock", 
            "text": "This will show charts about returns and shock events.  pf.create_returns_tear_sheet(stock_rets)  Entire data start date: 2012-05-21 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nBacktest Months: 38\n                   Backtest\nsortino_ratio          1.65\nomega_ratio            1.18\nmax_drawdown          -0.48\ncalmar_ratio           0.79\nannual_return          0.38\nstability              0.87\nsharpe_ratio           0.85\nannual_volatility      0.44\nalpha                  0.20\nbeta                   0.98\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              47.90  2012-05-21 00:00:00+00:00  2012-09-04 00:00:00+00:00   \n1              22.06  2014-03-10 00:00:00+00:00  2014-04-28 00:00:00+00:00   \n2              17.34  2013-10-18 00:00:00+00:00  2013-11-25 00:00:00+00:00   \n4               9.21  2014-10-28 00:00:00+00:00  2014-11-19 00:00:00+00:00   \n3               9.20  2015-03-24 00:00:00+00:00  2015-05-12 00:00:00+00:00\n\n               recovery date duration  \n0  2013-07-25 00:00:00+00:00      309  \n1  2014-07-24 00:00:00+00:00       99  \n2  2013-12-17 00:00:00+00:00       43  \n4  2014-12-22 00:00:00+00:00       40  \n3  2015-06-23 00:00:00+00:00       66\n\n\n2-sigma returns daily    -0.054\n2-sigma returns weekly   -0.110\ndtype: float64\n\n\n/opt/miniconda/lib/python2.7/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)", 
            "title": "Create a full tear sheet for the single stock"
        }, 
        {
            "location": "/single_stock_example/#create-a-full-tear-sheet-for-an-equal-weight-portfolio-of", 
            "text": "Long SPY  Short QQQ  Long GLD  Long TLT   Additionally, we set the live start date as an example.  stock_rets_SPY = pf.utils.get_symbol_rets('SPY')\nstock_rets_QQQ = pf.utils.get_symbol_rets('QQQ')\nstock_rets_GLD = pf.utils.get_symbol_rets('GLD')\nstock_rets_TLT = pf.utils.get_symbol_rets('TLT')  portfolio_4_assets_rets = pf.timeseries.portfolio_returns([stock_rets_SPY, -1 * stock_rets_QQQ, stock_rets_GLD, stock_rets_TLT], \n                                exclude_non_overlapping=True)  pf.create_full_tear_sheet(portfolio_4_assets_rets, live_start_date='2013-10-22')  Entire data start date: 2004-11-19 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nOut-of-Sample Months: 21\nBacktest Months: 106\n                   Backtest  Out_of_Sample  All_History\nsortino_ratio          0.96          -0.43         0.78\nomega_ratio            1.12           0.95         1.09\nmax_drawdown          -0.11          -0.08        -0.13\ncalmar_ratio           0.41          -0.20         0.28\nannual_return          0.05          -0.02         0.04\nstability              0.94           0.04         0.83\nsharpe_ratio           0.65          -0.28         0.53\nannual_volatility      0.07           0.06         0.07\nalpha                  0.05           0.00         0.04\nbeta                  -0.04          -0.17        -0.05\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              13.06  2012-11-13 00:00:00+00:00  2013-12-19 00:00:00+00:00   \n2               8.96  2008-03-17 00:00:00+00:00  2008-08-15 00:00:00+00:00   \n1               8.85  2008-12-30 00:00:00+00:00  2009-07-29 00:00:00+00:00   \n4               4.66  2010-10-06 00:00:00+00:00  2011-01-27 00:00:00+00:00   \n3               4.38  2012-01-31 00:00:00+00:00  2012-04-03 00:00:00+00:00\n\n               recovery date duration  \n0                        NaN      NaN  \n2  2008-12-10 00:00:00+00:00      193  \n1  2010-06-07 00:00:00+00:00      375  \n4  2011-04-27 00:00:00+00:00      146  \n3  2012-06-01 00:00:00+00:00       89\n\n\n2-sigma returns daily    -0.008\n2-sigma returns weekly   -0.016\ndtype: float64\n\nStress Events\n                                    mean    min    max\nLehmann                            0.001 -0.016  0.031\nUS downgrade/European Debt Crisis  0.002 -0.018  0.015\nFukushima                          0.001 -0.006  0.007\nEZB IR Event                       0.000 -0.007  0.004\nAug07                              0.000 -0.006  0.005\nSept08                             0.002 -0.016  0.031\n2009Q1                            -0.001 -0.012  0.015\n2009Q2                            -0.000 -0.012  0.020\nFlash Crash                        0.002 -0.009  0.015\nApr14                              0.000 -0.005  0.009\nOct14                             -0.000 -0.007  0.007", 
            "title": "Create a full tear sheet for an equal-weight portfolio of:"
        }, 
        {
            "location": "/zipline_algo_example/", 
            "text": "Zipline algorithm analysis example in pyfolio\n\n\nHere's an example where we run an algorithm with zipline, then produce tear sheets for that algorithm.\n\n\nImports\n\n\nImport pyfolio, along with the necessary modules for running our zipline backtest.\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/pandas/io/data.py:33: FutureWarning: \nThe pandas.io.data module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.\nAfter installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.\n  FutureWarning)\n\n\n\nimport numpy as np\nimport pandas as pd\n\nimport sys\nimport logbook\nimport numpy as np\nfrom datetime import datetime\nimport pytz\n\n# Import Zipline, the open source backtester\nfrom zipline import TradingAlgorithm\nfrom zipline.data.loader import load_bars_from_yahoo\nfrom zipline.api import order_target, symbol, history, add_history, schedule_function, date_rules, time_rules\nfrom zipline.algorithm import TradingAlgorithm\nfrom zipline.utils.factory import load_from_yahoo\nfrom zipline.finance import commission\n\n\n\n\nRun our zipline algorithm\n\n\nThis algorithm can also be adjusted to execute a modified, or completely different, trading strategy.\n\n\n# Zipline trading algorithm\n# Taken from zipline.examples.olmar\nzipline_logging = logbook.NestedSetup([\n    logbook.NullHandler(level=logbook.DEBUG),\n    logbook.StreamHandler(sys.stdout, level=logbook.INFO),\n    logbook.StreamHandler(sys.stderr, level=logbook.ERROR),\n])\nzipline_logging.push_application()\n\nSTOCKS = ['AMD', 'CERN', 'COST', 'DELL', 'GPS', 'INTC', 'MMM']\n\n\n# On-Line Portfolio Moving Average Reversion\n\n# More info can be found in the corresponding paper:\n# http://icml.cc/2012/papers/168.pdf\ndef initialize(algo, eps=1, window_length=5):\n    algo.stocks = STOCKS\n    algo.sids = [algo.symbol(symbol) for symbol in algo.stocks]\n    algo.m = len(algo.stocks)\n    algo.price = {}\n    algo.b_t = np.ones(algo.m) / algo.m\n    algo.last_desired_port = np.ones(algo.m) / algo.m\n    algo.eps = eps\n    algo.init = True\n    algo.days = 0\n    algo.window_length = window_length\n    algo.add_transform('mavg', 5)\n\n    algo.set_commission(commission.PerShare(cost=0))\n\n\ndef handle_data(algo, data):\n    algo.days += 1\n    if algo.days \n algo.window_length:\n        return\n\n    if algo.init:\n        rebalance_portfolio(algo, data, algo.b_t)\n        algo.init = False\n        return\n\n    m = algo.m\n\n    x_tilde = np.zeros(m)\n    b = np.zeros(m)\n\n    # find relative moving average price for each asset\n    for i, sid in enumerate(algo.sids):\n        price = data[sid].price\n        # Relative mean deviation\n        x_tilde[i] = data[sid].mavg(algo.window_length) / price\n\n    ###########################\n    # Inside of OLMAR (algo 2)\n    x_bar = x_tilde.mean()\n\n    # market relative deviation\n    mark_rel_dev = x_tilde - x_bar\n\n    # Expected return with current portfolio\n    exp_return = np.dot(algo.b_t, x_tilde)\n    weight = algo.eps - exp_return\n    variability = (np.linalg.norm(mark_rel_dev)) ** 2\n\n    # test for divide-by-zero case\n    if variability == 0.0:\n        step_size = 0\n    else:\n        step_size = max(0, weight / variability)\n\n    b = algo.b_t + step_size * mark_rel_dev\n    b_norm = simplex_projection(b)\n    np.testing.assert_almost_equal(b_norm.sum(), 1)\n\n    rebalance_portfolio(algo, data, b_norm)\n\n    # update portfolio\n    algo.b_t = b_norm\n\n\ndef rebalance_portfolio(algo, data, desired_port):\n    # rebalance portfolio\n    desired_amount = np.zeros_like(desired_port)\n    current_amount = np.zeros_like(desired_port)\n    prices = np.zeros_like(desired_port)\n\n    if algo.init:\n        positions_value = algo.portfolio.starting_cash\n    else:\n        positions_value = algo.portfolio.positions_value + \\\n            algo.portfolio.cash\n\n    for i, sid in enumerate(algo.sids):\n        current_amount[i] = algo.portfolio.positions[sid].amount\n        prices[i] = data[sid].price\n\n    desired_amount = np.round(desired_port * positions_value / prices)\n\n    algo.last_desired_port = desired_port\n    diff_amount = desired_amount - current_amount\n\n    for i, sid in enumerate(algo.sids):\n        algo.order(sid, diff_amount[i])\n\n\ndef simplex_projection(v, b=1):\n    \nProjection vectors to the simplex domain\n\n    Implemented according to the paper: Efficient projections onto the\n    l1-ball for learning in high dimensions, John Duchi, et al. ICML 2008.\n    Implementation Time: 2011 June 17 by Bin@libin AT pmail.ntu.edu.sg\n    Optimization Problem: min_{w}\\| w - v \\|_{2}^{2}\n    s.t. sum_{i=1}^{m}=z, w_{i}\\geq 0\n\n    Input: A vector v \\in R^{m}, and a scalar z \n 0 (default=1)\n    Output: Projection vector w\n\n    :Example:\n    \n proj = simplex_projection([.4 ,.3, -.4, .5])\n    \n print(proj)\n    array([ 0.33333333, 0.23333333, 0. , 0.43333333])\n    \n print(proj.sum())\n    1.0\n\n    Original matlab implementation: John Duchi (jduchi@cs.berkeley.edu)\n    Python-port: Copyright 2013 by Thomas Wiecki (thomas.wiecki@gmail.com).\n    \n\n\n    v = np.asarray(v)\n    p = len(v)\n\n    # Sort v into u in descending order\n    v = (v \n 0) * v\n    u = np.sort(v)[::-1]\n    sv = np.cumsum(u)\n\n    rho = np.where(u \n (sv - b) / np.arange(1, p + 1))[0][-1]\n    theta = np.max([0, (sv[rho] - b) / (rho + 1)])\n    w = (v - theta)\n    w[w \n 0] = 0\n    return w\n\nstart = datetime(2004, 1, 1, 0, 0, 0, 0, pytz.utc)\nend = datetime(2010, 1, 1, 0, 0, 0, 0, pytz.utc)\ndata = load_from_yahoo(stocks=STOCKS, indexes={}, start=start, end=end)\ndata = data.dropna()\nolmar = TradingAlgorithm(handle_data=handle_data,\n                         initialize=initialize,\n                         identifiers=STOCKS)\nbacktest = olmar.run(data)\n\n\n\n\nAMD\nCERN\nCOST\nDELL\nGPS\nINTC\nMMM\n[2015-10-21 14:10:42.252494] INFO: Performance: Simulated 1511 trading days out of 1511.\n[2015-10-21 14:10:42.253271] INFO: Performance: first open: 2004-01-02 14:31:00+00:00\n[2015-10-21 14:10:42.253904] INFO: Performance: last close: 2009-12-31 21:00:00+00:00\n\n\n\nExtract metrics\n\n\nGet the returns, positions, and transactions from the zipline backtest object.\n\n\nreturns, positions, transactions, gross_lev = pf.utils.extract_rets_pos_txn_from_zipline(backtest)\n\n\n\n\nSingle plot example\n\n\nMake one plot of the top 5 drawdown periods.\n\n\npf.plot_drawdown_periods(returns, top=5).set_xlabel('Date')\n\n\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\n\nmatplotlib.text.Text at 0x7f7287bb0860\n\n\n\n\n\n\nFull tear sheet example\n\n\nCreate a full tear sheet for our algorithm. As an example, set the live start date to something arbitrary.\n\n\npf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22')\n\n\n\n\nEntire data start date: 2004-01-02\nEntire data end date: 2009-12-31\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nannual_return          0.12           0.06         0.12\nannual_volatility      0.26           0.22         0.25\nsharpe_ratio           0.47           0.28         0.48\ncalmar_ratio           0.20           0.84         0.21\nstability              0.00           0.05         0.01\nmax_drawdown          -0.60          -0.07        -0.60\nomega_ratio            1.09           1.05         1.09\nsortino_ratio          0.70           0.39         0.71\nskewness               0.28          -0.28         0.27\nkurtosis               4.10           0.46         4.05\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              59.50 2007-11-06  2008-11-20           NaT      NaN\n1              22.33 2006-02-16  2006-08-31    2007-05-21      328\n2              12.52 2005-07-28  2005-10-12    2006-01-11      120\n3              11.28 2004-11-15  2005-04-28    2005-07-22      180\n4               9.44 2007-07-16  2007-08-06    2007-09-04       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\nStress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062\n\n\n\n\n\nTop 10 long positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nTop 10 short positions of all time (and max%)\n[]\n[]\n\n\nTop 10 positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nAll positions ever held\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\n\n\n\n\n\nSuppressing symbol output\n\n\nWhen sharing tear sheets it might be undesirable to display which symbols where used by a strategy. To suppress these in the tear sheet you can pass \nhide_positions=True\n.\n\n\npf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22',\n                          hide_positions=True)\n\n\n\n\nEntire data start date: 2004-01-02\nEntire data end date: 2009-12-31\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nannual_return          0.12           0.06         0.12\nannual_volatility      0.26           0.22         0.25\nsharpe_ratio           0.47           0.28         0.48\ncalmar_ratio           0.20           0.84         0.21\nstability              0.00           0.05         0.01\nmax_drawdown          -0.60          -0.07        -0.60\nomega_ratio            1.09           1.05         1.09\nsortino_ratio          0.70           0.39         0.71\nskewness               0.28          -0.28         0.27\nkurtosis               4.10           0.46         4.05\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              59.50 2007-11-06  2008-11-20           NaT      NaN\n1              22.33 2006-02-16  2006-08-31    2007-05-21      328\n2              12.52 2005-07-28  2005-10-12    2006-01-11      120\n3              11.28 2004-11-15  2005-04-28    2005-07-22      180\n4               9.44 2007-07-16  2007-08-06    2007-09-04       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\nStress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062", 
            "title": "Zipline algorithm"
        }, 
        {
            "location": "/zipline_algo_example/#zipline-algorithm-analysis-example-in-pyfolio", 
            "text": "Here's an example where we run an algorithm with zipline, then produce tear sheets for that algorithm.", 
            "title": "Zipline algorithm analysis example in pyfolio"
        }, 
        {
            "location": "/zipline_algo_example/#imports", 
            "text": "Import pyfolio, along with the necessary modules for running our zipline backtest.  %matplotlib inline\nimport pyfolio as pf  /home/wiecki/miniconda3/lib/python3.4/site-packages/pandas/io/data.py:33: FutureWarning: \nThe pandas.io.data module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.\nAfter installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.\n  FutureWarning)  import numpy as np\nimport pandas as pd\n\nimport sys\nimport logbook\nimport numpy as np\nfrom datetime import datetime\nimport pytz\n\n# Import Zipline, the open source backtester\nfrom zipline import TradingAlgorithm\nfrom zipline.data.loader import load_bars_from_yahoo\nfrom zipline.api import order_target, symbol, history, add_history, schedule_function, date_rules, time_rules\nfrom zipline.algorithm import TradingAlgorithm\nfrom zipline.utils.factory import load_from_yahoo\nfrom zipline.finance import commission", 
            "title": "Imports"
        }, 
        {
            "location": "/zipline_algo_example/#run-our-zipline-algorithm", 
            "text": "This algorithm can also be adjusted to execute a modified, or completely different, trading strategy.  # Zipline trading algorithm\n# Taken from zipline.examples.olmar\nzipline_logging = logbook.NestedSetup([\n    logbook.NullHandler(level=logbook.DEBUG),\n    logbook.StreamHandler(sys.stdout, level=logbook.INFO),\n    logbook.StreamHandler(sys.stderr, level=logbook.ERROR),\n])\nzipline_logging.push_application()\n\nSTOCKS = ['AMD', 'CERN', 'COST', 'DELL', 'GPS', 'INTC', 'MMM']\n\n\n# On-Line Portfolio Moving Average Reversion\n\n# More info can be found in the corresponding paper:\n# http://icml.cc/2012/papers/168.pdf\ndef initialize(algo, eps=1, window_length=5):\n    algo.stocks = STOCKS\n    algo.sids = [algo.symbol(symbol) for symbol in algo.stocks]\n    algo.m = len(algo.stocks)\n    algo.price = {}\n    algo.b_t = np.ones(algo.m) / algo.m\n    algo.last_desired_port = np.ones(algo.m) / algo.m\n    algo.eps = eps\n    algo.init = True\n    algo.days = 0\n    algo.window_length = window_length\n    algo.add_transform('mavg', 5)\n\n    algo.set_commission(commission.PerShare(cost=0))\n\n\ndef handle_data(algo, data):\n    algo.days += 1\n    if algo.days   algo.window_length:\n        return\n\n    if algo.init:\n        rebalance_portfolio(algo, data, algo.b_t)\n        algo.init = False\n        return\n\n    m = algo.m\n\n    x_tilde = np.zeros(m)\n    b = np.zeros(m)\n\n    # find relative moving average price for each asset\n    for i, sid in enumerate(algo.sids):\n        price = data[sid].price\n        # Relative mean deviation\n        x_tilde[i] = data[sid].mavg(algo.window_length) / price\n\n    ###########################\n    # Inside of OLMAR (algo 2)\n    x_bar = x_tilde.mean()\n\n    # market relative deviation\n    mark_rel_dev = x_tilde - x_bar\n\n    # Expected return with current portfolio\n    exp_return = np.dot(algo.b_t, x_tilde)\n    weight = algo.eps - exp_return\n    variability = (np.linalg.norm(mark_rel_dev)) ** 2\n\n    # test for divide-by-zero case\n    if variability == 0.0:\n        step_size = 0\n    else:\n        step_size = max(0, weight / variability)\n\n    b = algo.b_t + step_size * mark_rel_dev\n    b_norm = simplex_projection(b)\n    np.testing.assert_almost_equal(b_norm.sum(), 1)\n\n    rebalance_portfolio(algo, data, b_norm)\n\n    # update portfolio\n    algo.b_t = b_norm\n\n\ndef rebalance_portfolio(algo, data, desired_port):\n    # rebalance portfolio\n    desired_amount = np.zeros_like(desired_port)\n    current_amount = np.zeros_like(desired_port)\n    prices = np.zeros_like(desired_port)\n\n    if algo.init:\n        positions_value = algo.portfolio.starting_cash\n    else:\n        positions_value = algo.portfolio.positions_value + \\\n            algo.portfolio.cash\n\n    for i, sid in enumerate(algo.sids):\n        current_amount[i] = algo.portfolio.positions[sid].amount\n        prices[i] = data[sid].price\n\n    desired_amount = np.round(desired_port * positions_value / prices)\n\n    algo.last_desired_port = desired_port\n    diff_amount = desired_amount - current_amount\n\n    for i, sid in enumerate(algo.sids):\n        algo.order(sid, diff_amount[i])\n\n\ndef simplex_projection(v, b=1):\n     Projection vectors to the simplex domain\n\n    Implemented according to the paper: Efficient projections onto the\n    l1-ball for learning in high dimensions, John Duchi, et al. ICML 2008.\n    Implementation Time: 2011 June 17 by Bin@libin AT pmail.ntu.edu.sg\n    Optimization Problem: min_{w}\\| w - v \\|_{2}^{2}\n    s.t. sum_{i=1}^{m}=z, w_{i}\\geq 0\n\n    Input: A vector v \\in R^{m}, and a scalar z   0 (default=1)\n    Output: Projection vector w\n\n    :Example:\n      proj = simplex_projection([.4 ,.3, -.4, .5])\n      print(proj)\n    array([ 0.33333333, 0.23333333, 0. , 0.43333333])\n      print(proj.sum())\n    1.0\n\n    Original matlab implementation: John Duchi (jduchi@cs.berkeley.edu)\n    Python-port: Copyright 2013 by Thomas Wiecki (thomas.wiecki@gmail.com).\n     \n\n    v = np.asarray(v)\n    p = len(v)\n\n    # Sort v into u in descending order\n    v = (v   0) * v\n    u = np.sort(v)[::-1]\n    sv = np.cumsum(u)\n\n    rho = np.where(u   (sv - b) / np.arange(1, p + 1))[0][-1]\n    theta = np.max([0, (sv[rho] - b) / (rho + 1)])\n    w = (v - theta)\n    w[w   0] = 0\n    return w\n\nstart = datetime(2004, 1, 1, 0, 0, 0, 0, pytz.utc)\nend = datetime(2010, 1, 1, 0, 0, 0, 0, pytz.utc)\ndata = load_from_yahoo(stocks=STOCKS, indexes={}, start=start, end=end)\ndata = data.dropna()\nolmar = TradingAlgorithm(handle_data=handle_data,\n                         initialize=initialize,\n                         identifiers=STOCKS)\nbacktest = olmar.run(data)  AMD\nCERN\nCOST\nDELL\nGPS\nINTC\nMMM\n[2015-10-21 14:10:42.252494] INFO: Performance: Simulated 1511 trading days out of 1511.\n[2015-10-21 14:10:42.253271] INFO: Performance: first open: 2004-01-02 14:31:00+00:00\n[2015-10-21 14:10:42.253904] INFO: Performance: last close: 2009-12-31 21:00:00+00:00", 
            "title": "Run our zipline algorithm"
        }, 
        {
            "location": "/zipline_algo_example/#extract-metrics", 
            "text": "Get the returns, positions, and transactions from the zipline backtest object.  returns, positions, transactions, gross_lev = pf.utils.extract_rets_pos_txn_from_zipline(backtest)", 
            "title": "Extract metrics"
        }, 
        {
            "location": "/zipline_algo_example/#single-plot-example", 
            "text": "Make one plot of the top 5 drawdown periods.  pf.plot_drawdown_periods(returns, top=5).set_xlabel('Date')  /home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1) matplotlib.text.Text at 0x7f7287bb0860", 
            "title": "Single plot example"
        }, 
        {
            "location": "/zipline_algo_example/#full-tear-sheet-example", 
            "text": "Create a full tear sheet for our algorithm. As an example, set the live start date to something arbitrary.  pf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22')  Entire data start date: 2004-01-02\nEntire data end date: 2009-12-31\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nannual_return          0.12           0.06         0.12\nannual_volatility      0.26           0.22         0.25\nsharpe_ratio           0.47           0.28         0.48\ncalmar_ratio           0.20           0.84         0.21\nstability              0.00           0.05         0.01\nmax_drawdown          -0.60          -0.07        -0.60\nomega_ratio            1.09           1.05         1.09\nsortino_ratio          0.70           0.39         0.71\nskewness               0.28          -0.28         0.27\nkurtosis               4.10           0.46         4.05\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              59.50 2007-11-06  2008-11-20           NaT      NaN\n1              22.33 2006-02-16  2006-08-31    2007-05-21      328\n2              12.52 2005-07-28  2005-10-12    2006-01-11      120\n3              11.28 2004-11-15  2005-04-28    2005-07-22      180\n4               9.44 2007-07-16  2007-08-06    2007-09-04       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)   Stress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062   Top 10 long positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nTop 10 short positions of all time (and max%)\n[]\n[]\n\n\nTop 10 positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nAll positions ever held\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]", 
            "title": "Full tear sheet example"
        }, 
        {
            "location": "/zipline_algo_example/#suppressing-symbol-output", 
            "text": "When sharing tear sheets it might be undesirable to display which symbols where used by a strategy. To suppress these in the tear sheet you can pass  hide_positions=True .  pf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22',\n                          hide_positions=True)  Entire data start date: 2004-01-02\nEntire data end date: 2009-12-31\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nannual_return          0.12           0.06         0.12\nannual_volatility      0.26           0.22         0.25\nsharpe_ratio           0.47           0.28         0.48\ncalmar_ratio           0.20           0.84         0.21\nstability              0.00           0.05         0.01\nmax_drawdown          -0.60          -0.07        -0.60\nomega_ratio            1.09           1.05         1.09\nsortino_ratio          0.70           0.39         0.71\nskewness               0.28          -0.28         0.27\nkurtosis               4.10           0.46         4.05\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              59.50 2007-11-06  2008-11-20           NaT      NaN\n1              22.33 2006-02-16  2006-08-31    2007-05-21      328\n2              12.52 2005-07-28  2005-10-12    2006-01-11      120\n3              11.28 2004-11-15  2005-04-28    2005-07-22      180\n4               9.44 2007-07-16  2007-08-06    2007-09-04       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)   Stress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062", 
            "title": "Suppressing symbol output"
        }, 
        {
            "location": "/bayesian/", 
            "text": "Bayesian performance analysis example in pyfolio\n\n\nThere are also a few more advanced (and still experimental) analysis methods in pyfolio based on Bayesian statistics. \n\n\nThe main benefit of these methods is \nuncertainty quantification\n. All the values you saw above, like the Sharpe ratio, are just single numbers. These estimates are noisy because they have been computed over a limited number of data points. So how much can you trust these numbers? You don't know because there is no sense of uncertainty. That is where Bayesian statistics helps as instead of single values, we are dealing with probability distributions that assign degrees of belief to all possible parameter values.\n\n\nLets create the Bayesian tear sheet. Under the hood this is running MCMC sampling in \nPyMC3\n to estimate the posteriors which can take quite a while (that's the reason why we don't generate this by default in \ncreate_full_tear_sheet()\n).\n\n\nImport pyfolio\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\nFetch the daily returns for a stock\n\n\n#stock_rets = pf.utils.get_symbol_rets('FB')\nimport pandas as pd\nstock_rets = pd.read_pickle('fb.pickle')\n\n\n\n\nCreate Bayesian tear sheet\n\n\nout_of_sample = stock_rets.index[-40]\n\n\n\n\n%pdb\n\n\n\n\nAutomatic pdb calling has been turned ON\n\n\n\npf.create_bayesian_tear_sheet(stock_rets, live_start_date=out_of_sample)\n\n\n\n\nRunning T model\n [-----------------100%-----------------] 2000 of 2000 complete in 3.6 sec\nFinished T model (required 51.53 seconds).\n\nRunning BEST model\n [-----------------100%-----------------] 2000 of 2000 complete in 5.7 sec\nFinished BEST model (required 51.95 seconds).\n\nFinished plotting Bayesian cone (required 0.13 seconds).\n\nFinished plotting BEST results (required 1.18 seconds).\n\nFinished computing Bayesian predictions (required 0.32 seconds).\n\nFinished plotting Bayesian VaRs estimate (required 0.13 seconds).\n\nRunning alpha beta model\n [-----------------100%-----------------] 2000 of 2000 complete in 3.5 sec\nFinished running alpha beta model (required 58.74 seconds).\n\nFinished plotting alpha beta model (required 0.22 seconds).\n\nTotal runtime was 164.22 seconds.\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/axes/_axes.py:475: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n  warnings.warn(\"No labelled objects found. \"\n\n\n\n\n\nLets go through these row by row:\n\n\n\n\nThe first one is the Bayesian cone plot that is the result of a summer internship project of Sepideh Sadeghi here at Quantopian. It's similar to the cone plot you already saw in the tear sheet above but has two critical additions: (i) it takes uncertainty into account (i.e. a short backtest length will result in a wider cone), and (ii) it does not assume normality of returns but instead uses a \nStudent-T distribution\n with heavier tails.\n\n\nThe next row compares mean returns of the in-sample (backest) and out-of-sample or OOS (forward) period. As you can see, mean returns are not a single number but a (posterior) distribution that gives us an indication of how certain we can be in our estimates. The green distribution on the left side is much wider, representing our increased uncertainty due to having less OOS data. We can then calculate the difference between these two distributions as shown on the right side. The grey lines denote the 2.5% and 97.5% percentiles. Intuitively, if the right grey line is lower than 0 you can say that with probability \n 97.5% the OOS mean returns are below what is suggested by the backtest. The model used here is called \nBEST\n and was developed by John Kruschke.\n\n\nThe next couple of rows follow the same pattern but are an estimate of annual volatility, Sharpe ratio and their respective differences.\n\n\nThe 5th row shows the effect size or the difference of means normalized by the standard deviation and gives you a general sense how far apart the two distributions are. Intuitively, even if the means are significantly different, it may not be very meaningful if the standard deviation is huge amounting to a tiny difference of the two returns distributions.\n\n\nThe 6th row shows predicted returns (based on the backtest) for tomorrow, and 5 days from now. The blue line indicates the probability of losing more than 5% of your portfolio value and can be interpeted as a Bayesian VaR estimate.\n\n\nThe 7th row shows a Bayesian estimate of annual alpha and beta. In addition to uncertainty estimates, this model, like all above ones, assumes returns to be T-distributed which leads to more robust estimates than a standard linear regression would.  The default benchmark is the S\nP500.  Alternatively, users may use the Fama-French model as a bunchmark by setting benchmark_rets=\"Fama-French\".  \n\n\n[only present if stoch_vol=True] The 8th row shows Bayesian estimates for log(sigma) and log(nu), two parameters of the \nstochastic volatility model\n.\n\n\n[only present if stoch_vol=True] The 9th row shows the volatility measured by the stochastic voatility model, overlaid on the absolute value of the returns.\n\n\nBy default, stoch_vol=False because running the stochastic volatility model is computationally expensive.\n\n\nOnly the most recent 400 days of returns are used when computing the stochastic volatility model.  This is to minimize computational time.\n\n\n\n\nRunning models directly\n\n\nYou can also run individual models. All models can be found in \npyfolio.bayesian\n and run via the \nrun_model()\n function.\n\n\nhelp(pf.bayesian.run_model)\n\n\n\n\nHelp on function run_model in module pyfolio.bayesian:\n\nrun_model(model, returns_train, returns_test=None, bmark=None, samples=500, ppc=False)\n    Run one of the Bayesian models.\n\n    Parameters\n    ----------\n    model : {'alpha_beta', 't', 'normal', 'best'}\n        Which model to run\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series (optional)\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    bmark : pd.Series or pd.DataFrame (optional)\n        Only used for alpha_beta to estimate regression coefficients.\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n\n    Returns\n    -------\n    pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n\n\nFor example, to run a model that assumes returns to be normally distributed, you can call:\n\n\n# Run model that assumes returns to be T-distributed\ntrace = pf.bayesian.run_model('t', stock_rets)\n\n\n\n\n [-----------------100%-----------------] 500 of 500 complete in 1.5 sec\n\n\n\nThe returned trace object can be directly inquired. For example might we ask what the probability of the Sharpe ratio being larger than 0 is by checking what percentage of posterior samples of the Sharpe ratio are \n 0:\n\n\n# Check what frequency of samples from the sharpe posterior are above 0.\nprint('Probability of Sharpe ratio \n 0 = {:3}%'.format((trace['sharpe'] \n 0).mean() * 100))\n\n\n\n\nProbability of Sharpe ratio \n 0 = 80.4%\n\n\n\nBut we can also interact with it like with any other \npymc3\n trace:\n\n\nimport pymc3 as pm\npm.traceplot(trace);\n\n\n\n\n\n\nFurther reading\n\n\nFor more information on Bayesian statistics, check out these resources:\n\n\n\n\nA blog post about the Bayesian models with Sepideh Sadeghi\n\n\nMy personal blog on Bayesian modeling\n\n\nA talk I gave in Singapore on \nProbabilistic Programming in Quantitative Finance\n\n\nThe IPython NB book \nBayesian Methods for Hackers\n.", 
            "title": "Bayesian analysis"
        }, 
        {
            "location": "/bayesian/#bayesian-performance-analysis-example-in-pyfolio", 
            "text": "There are also a few more advanced (and still experimental) analysis methods in pyfolio based on Bayesian statistics.   The main benefit of these methods is  uncertainty quantification . All the values you saw above, like the Sharpe ratio, are just single numbers. These estimates are noisy because they have been computed over a limited number of data points. So how much can you trust these numbers? You don't know because there is no sense of uncertainty. That is where Bayesian statistics helps as instead of single values, we are dealing with probability distributions that assign degrees of belief to all possible parameter values.  Lets create the Bayesian tear sheet. Under the hood this is running MCMC sampling in  PyMC3  to estimate the posteriors which can take quite a while (that's the reason why we don't generate this by default in  create_full_tear_sheet() ).", 
            "title": "Bayesian performance analysis example in pyfolio"
        }, 
        {
            "location": "/bayesian/#import-pyfolio", 
            "text": "%matplotlib inline\nimport pyfolio as pf", 
            "title": "Import pyfolio"
        }, 
        {
            "location": "/bayesian/#fetch-the-daily-returns-for-a-stock", 
            "text": "#stock_rets = pf.utils.get_symbol_rets('FB')\nimport pandas as pd\nstock_rets = pd.read_pickle('fb.pickle')", 
            "title": "Fetch the daily returns for a stock"
        }, 
        {
            "location": "/bayesian/#create-bayesian-tear-sheet", 
            "text": "out_of_sample = stock_rets.index[-40]  %pdb  Automatic pdb calling has been turned ON  pf.create_bayesian_tear_sheet(stock_rets, live_start_date=out_of_sample)  Running T model\n [-----------------100%-----------------] 2000 of 2000 complete in 3.6 sec\nFinished T model (required 51.53 seconds).\n\nRunning BEST model\n [-----------------100%-----------------] 2000 of 2000 complete in 5.7 sec\nFinished BEST model (required 51.95 seconds).\n\nFinished plotting Bayesian cone (required 0.13 seconds).\n\nFinished plotting BEST results (required 1.18 seconds).\n\nFinished computing Bayesian predictions (required 0.32 seconds).\n\nFinished plotting Bayesian VaRs estimate (required 0.13 seconds).\n\nRunning alpha beta model\n [-----------------100%-----------------] 2000 of 2000 complete in 3.5 sec\nFinished running alpha beta model (required 58.74 seconds).\n\nFinished plotting alpha beta model (required 0.22 seconds).\n\nTotal runtime was 164.22 seconds.\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/axes/_axes.py:475: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n  warnings.warn(\"No labelled objects found. \"   Lets go through these row by row:   The first one is the Bayesian cone plot that is the result of a summer internship project of Sepideh Sadeghi here at Quantopian. It's similar to the cone plot you already saw in the tear sheet above but has two critical additions: (i) it takes uncertainty into account (i.e. a short backtest length will result in a wider cone), and (ii) it does not assume normality of returns but instead uses a  Student-T distribution  with heavier tails.  The next row compares mean returns of the in-sample (backest) and out-of-sample or OOS (forward) period. As you can see, mean returns are not a single number but a (posterior) distribution that gives us an indication of how certain we can be in our estimates. The green distribution on the left side is much wider, representing our increased uncertainty due to having less OOS data. We can then calculate the difference between these two distributions as shown on the right side. The grey lines denote the 2.5% and 97.5% percentiles. Intuitively, if the right grey line is lower than 0 you can say that with probability   97.5% the OOS mean returns are below what is suggested by the backtest. The model used here is called  BEST  and was developed by John Kruschke.  The next couple of rows follow the same pattern but are an estimate of annual volatility, Sharpe ratio and their respective differences.  The 5th row shows the effect size or the difference of means normalized by the standard deviation and gives you a general sense how far apart the two distributions are. Intuitively, even if the means are significantly different, it may not be very meaningful if the standard deviation is huge amounting to a tiny difference of the two returns distributions.  The 6th row shows predicted returns (based on the backtest) for tomorrow, and 5 days from now. The blue line indicates the probability of losing more than 5% of your portfolio value and can be interpeted as a Bayesian VaR estimate.  The 7th row shows a Bayesian estimate of annual alpha and beta. In addition to uncertainty estimates, this model, like all above ones, assumes returns to be T-distributed which leads to more robust estimates than a standard linear regression would.  The default benchmark is the S P500.  Alternatively, users may use the Fama-French model as a bunchmark by setting benchmark_rets=\"Fama-French\".    [only present if stoch_vol=True] The 8th row shows Bayesian estimates for log(sigma) and log(nu), two parameters of the  stochastic volatility model .  [only present if stoch_vol=True] The 9th row shows the volatility measured by the stochastic voatility model, overlaid on the absolute value of the returns.  By default, stoch_vol=False because running the stochastic volatility model is computationally expensive.  Only the most recent 400 days of returns are used when computing the stochastic volatility model.  This is to minimize computational time.", 
            "title": "Create Bayesian tear sheet"
        }, 
        {
            "location": "/bayesian/#running-models-directly", 
            "text": "You can also run individual models. All models can be found in  pyfolio.bayesian  and run via the  run_model()  function.  help(pf.bayesian.run_model)  Help on function run_model in module pyfolio.bayesian:\n\nrun_model(model, returns_train, returns_test=None, bmark=None, samples=500, ppc=False)\n    Run one of the Bayesian models.\n\n    Parameters\n    ----------\n    model : {'alpha_beta', 't', 'normal', 'best'}\n        Which model to run\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series (optional)\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    bmark : pd.Series or pd.DataFrame (optional)\n        Only used for alpha_beta to estimate regression coefficients.\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n\n    Returns\n    -------\n    pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.  For example, to run a model that assumes returns to be normally distributed, you can call:  # Run model that assumes returns to be T-distributed\ntrace = pf.bayesian.run_model('t', stock_rets)   [-----------------100%-----------------] 500 of 500 complete in 1.5 sec  The returned trace object can be directly inquired. For example might we ask what the probability of the Sharpe ratio being larger than 0 is by checking what percentage of posterior samples of the Sharpe ratio are   0:  # Check what frequency of samples from the sharpe posterior are above 0.\nprint('Probability of Sharpe ratio   0 = {:3}%'.format((trace['sharpe']   0).mean() * 100))  Probability of Sharpe ratio   0 = 80.4%  But we can also interact with it like with any other  pymc3  trace:  import pymc3 as pm\npm.traceplot(trace);", 
            "title": "Running models directly"
        }, 
        {
            "location": "/bayesian/#further-reading", 
            "text": "For more information on Bayesian statistics, check out these resources:   A blog post about the Bayesian models with Sepideh Sadeghi  My personal blog on Bayesian modeling  A talk I gave in Singapore on  Probabilistic Programming in Quantitative Finance  The IPython NB book  Bayesian Methods for Hackers .", 
            "title": "Further reading"
        }, 
        {
            "location": "/sector_mapping_example/", 
            "text": "Sector Mappings\n\n\nTo generate sector allocation plots in the positions tearsheet and PnL by sector in the round trips tearsheet, you must pass pyfolio a dictionary (or dict-like data struction) of symbol-sector mappings, where symbols are keys and sectors are values. \ncreate_full_tearsheet\n will also take symbol-sector mappings as keyword argument \nsector_mappings\n.\n\n\n%matplotlib inline\nimport pyfolio as pf\nimport gzip\nimport os\nimport pandas as pd\n\n\n\n\ntransactions = pd.read_csv(gzip.open('../tests/test_data/test_txn.csv.gz'),\n                    index_col=0, parse_dates=0)\npositions = pd.read_csv(gzip.open('../tests/test_data/test_pos.csv.gz'),\n                    index_col=0, parse_dates=0)\nreturns = pd.read_csv(gzip.open('../tests/test_data/test_returns.csv.gz'),\n                    index_col=0, parse_dates=0, header=None)[1]\ngross_lev = pd.read_csv(gzip.open('../tests/test_data/test_gross_lev.csv.gz'),\n                    index_col=0, parse_dates=0, header=None)[1]\n\n\n\n\nreturns.index = returns.index.tz_localize(\nUTC\n)\npositions.index = positions.index.tz_localize(\nUTC\n)\ntransactions.index = transactions.index.tz_localize(\nUTC\n)\ngross_lev.index = gross_lev.index.tz_localize(\nUTC\n)\n\n\n\n\n\npositions.head(2)\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nAMD\n\n      \nCERN\n\n      \nCOST\n\n      \nDELL\n\n      \nGPS\n\n      \nINTC\n\n      \nMMM\n\n      \ncash\n\n    \n\n    \n\n      \nindex\n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \n2004-01-09 00:00:00+00:00\n\n      \n6961.92\n\n      \n21017.07875\n\n      \n7282.266152\n\n      \n21264.55188\n\n      \n7091.080020\n\n      \n21259.33389\n\n      \n21316.129606\n\n      \n-6192.360298\n\n    \n\n    \n\n      \n2004-01-12 00:00:00+00:00\n\n      \n18198.58\n\n      \n18071.25000\n\n      \n17675.836401\n\n      \n10804.31924\n\n      \n10685.411865\n\n      \n17872.47748\n\n      \n10882.026400\n\n      \n-3329.289887\n\n    \n\n  \n\n\n\n\n\n\n\nsect_map = {'COST': 'Consumer Goods', \n            'INTC': 'Technology', \n            'CERN': 'Healthcare', \n            'GPS': 'Technology',\n            'MMM': 'Construction', \n            'DELL': 'Technology', \n            'AMD': 'Technology'}\n\n\n\n\npf.create_position_tear_sheet(returns, positions, gross_lev=gross_lev, sector_mappings=sect_map)\n\n\n\n\n\nTop 10 long positions of all time (and max%)\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\nTop 10 short positions of all time (and max%)\n['AMD' 'DELL' 'CERN' 'MMM' 'GPS' 'INTC' 'COST']\n[-0.301 -0.266 -0.255 -0.226 -0.201 -0.185 -0.164]\n\n\nTop 10 positions of all time (and max%)\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\nAll positions ever held\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\n\n\n\npf.create_round_trip_tear_sheet(positions, transactions, sector_mappings=sect_map)\n\n\n\n\n                      duration           pnl      returns      long\ncount                     1430   1430.000000  1430.000000      1430\nmean    9 days 16:40:56.154545     45.737238     0.003543  0.523077\nstd    22 days 02:16:41.165898   1616.537844     0.031288  0.499642\nmin            0 days 00:00:00 -30697.460000    -0.218045     False\n25%            0 days 23:59:59     -5.773144    -0.011450         0\n50%            2 days 23:59:59      0.871629     0.003885         1\n75%            5 days 23:59:59     40.438366     0.018126         1\nmax          286 days 00:00:00  17835.869482     0.204385      True\nPercent of round trips profitable = 57.2%\nMean return per winning round trip = 0.02181\nMean return per losing round trip = -0.02108\nA decision is made every 1.053 days.\n0.9495 trading decisions per day.\n19.94 trading decisions per month.\n\nProfitability (PnL / PnL total) per name:\nsymbol\nCOST    0.398964\nINTC    0.382659\nCERN    0.323077\nMMM     0.221479\nGPS     0.049385\nAMD    -0.064091\nDELL   -0.311473\nName: pnl, dtype: float64\n\nProfitability (PnL / PnL total) per name:\nsymbol\nConsumer Goods    0.398964\nHealthcare        0.323077\nConstruction      0.221479\nTechnology        0.056480\nName: pnl, dtype: float64\n\n\n\n\nmatplotlib.figure.Figure at 0x109d0f650", 
            "title": "Sector analysis"
        }, 
        {
            "location": "/sector_mapping_example/#sector-mappings", 
            "text": "To generate sector allocation plots in the positions tearsheet and PnL by sector in the round trips tearsheet, you must pass pyfolio a dictionary (or dict-like data struction) of symbol-sector mappings, where symbols are keys and sectors are values.  create_full_tearsheet  will also take symbol-sector mappings as keyword argument  sector_mappings .  %matplotlib inline\nimport pyfolio as pf\nimport gzip\nimport os\nimport pandas as pd  transactions = pd.read_csv(gzip.open('../tests/test_data/test_txn.csv.gz'),\n                    index_col=0, parse_dates=0)\npositions = pd.read_csv(gzip.open('../tests/test_data/test_pos.csv.gz'),\n                    index_col=0, parse_dates=0)\nreturns = pd.read_csv(gzip.open('../tests/test_data/test_returns.csv.gz'),\n                    index_col=0, parse_dates=0, header=None)[1]\ngross_lev = pd.read_csv(gzip.open('../tests/test_data/test_gross_lev.csv.gz'),\n                    index_col=0, parse_dates=0, header=None)[1]  returns.index = returns.index.tz_localize( UTC )\npositions.index = positions.index.tz_localize( UTC )\ntransactions.index = transactions.index.tz_localize( UTC )\ngross_lev.index = gross_lev.index.tz_localize( UTC )  positions.head(2)   \n   \n     \n       \n       AMD \n       CERN \n       COST \n       DELL \n       GPS \n       INTC \n       MMM \n       cash \n     \n     \n       index \n       \n       \n       \n       \n       \n       \n       \n       \n     \n   \n   \n     \n       2004-01-09 00:00:00+00:00 \n       6961.92 \n       21017.07875 \n       7282.266152 \n       21264.55188 \n       7091.080020 \n       21259.33389 \n       21316.129606 \n       -6192.360298 \n     \n     \n       2004-01-12 00:00:00+00:00 \n       18198.58 \n       18071.25000 \n       17675.836401 \n       10804.31924 \n       10685.411865 \n       17872.47748 \n       10882.026400 \n       -3329.289887 \n     \n      sect_map = {'COST': 'Consumer Goods', \n            'INTC': 'Technology', \n            'CERN': 'Healthcare', \n            'GPS': 'Technology',\n            'MMM': 'Construction', \n            'DELL': 'Technology', \n            'AMD': 'Technology'}  pf.create_position_tear_sheet(returns, positions, gross_lev=gross_lev, sector_mappings=sect_map)  Top 10 long positions of all time (and max%)\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\nTop 10 short positions of all time (and max%)\n['AMD' 'DELL' 'CERN' 'MMM' 'GPS' 'INTC' 'COST']\n[-0.301 -0.266 -0.255 -0.226 -0.201 -0.185 -0.164]\n\n\nTop 10 positions of all time (and max%)\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\nAll positions ever held\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]   pf.create_round_trip_tear_sheet(positions, transactions, sector_mappings=sect_map)                        duration           pnl      returns      long\ncount                     1430   1430.000000  1430.000000      1430\nmean    9 days 16:40:56.154545     45.737238     0.003543  0.523077\nstd    22 days 02:16:41.165898   1616.537844     0.031288  0.499642\nmin            0 days 00:00:00 -30697.460000    -0.218045     False\n25%            0 days 23:59:59     -5.773144    -0.011450         0\n50%            2 days 23:59:59      0.871629     0.003885         1\n75%            5 days 23:59:59     40.438366     0.018126         1\nmax          286 days 00:00:00  17835.869482     0.204385      True\nPercent of round trips profitable = 57.2%\nMean return per winning round trip = 0.02181\nMean return per losing round trip = -0.02108\nA decision is made every 1.053 days.\n0.9495 trading decisions per day.\n19.94 trading decisions per month.\n\nProfitability (PnL / PnL total) per name:\nsymbol\nCOST    0.398964\nINTC    0.382659\nCERN    0.323077\nMMM     0.221479\nGPS     0.049385\nAMD    -0.064091\nDELL   -0.311473\nName: pnl, dtype: float64\n\nProfitability (PnL / PnL total) per name:\nsymbol\nConsumer Goods    0.398964\nHealthcare        0.323077\nConstruction      0.221479\nTechnology        0.056480\nName: pnl, dtype: float64 matplotlib.figure.Figure at 0x109d0f650", 
            "title": "Sector Mappings"
        }, 
        {
            "location": "/round_trip_example/", 
            "text": "Round Trip Tearsheet\n\n\nWhen evaluating the performance of an investing strategy, it is helpful to quantify the frequency, duration, and profitability of its independent bets, or \"round trip\" trades. A round trip trade is started when a new long or short position is opened and is only completed when the number of shares in that position returns to or crosses zero. \n\n\nThe intent of the round trip tearsheet is to help differentiate strategies that profited off a few lucky trades from strategies that profited repeatedly from genuine alpha. Breaking down round trip profitability by traded name and sector can also help inform universe selection and identify exposure risks. For example, even if your equity curve looks robust, if only two securities in your universe of fifteen names contributed to overall profitability, you may have reason to question the logic of your strategy.\n\n\nTo identify round trips, pyfolio groups transactions by symbol and identifies the points at which each position amount leaves and returns to zero. Behind the scenes, transactions that cause position amounts to flip directly from long to short or short to long are divided into separate transactions so that distinct round trips can be identified. In calculating round trips, pyfolio will also append position closing transactions at the last timestamp in the positions data. This closing transaction will cause the PnL from any open positions to realized as completed round trips.\n\n\nNote: The round trip method of quantifying performance is not applicable to every style of strategy.\n For instance, simple rebalancing algorithms make very few, if any, round trip trades. The results of the round trip tearsheet will be less informative for any strategy that doesn't entirely exit positions.\n\n\n%matplotlib inline\nimport gzip\nimport os\nimport pandas as pd\nimport pyfolio as pf\n\n\n\n\ntransactions = pd.read_csv(gzip.open('../tests/test_data/test_txn.csv.gz'),\n                    index_col=0, parse_dates=0)\npositions = pd.read_csv(gzip.open('../tests/test_data/test_pos.csv.gz'),\n                    index_col=0, parse_dates=0)\nreturns = pd.read_csv(gzip.open('../tests/test_data/test_returns.csv.gz'),\n                    index_col=0, parse_dates=0)\n\n\n\n\n# Optional: Sector mappings may be passed in as a dict or pd.Series. If a mapping is\n# provided, PnL from symbols with mappings will be summed to display profitability by sector.\nsect_map = {'COST': 'Consumer Goods', 'INTC':'Technology', 'CERN':'Healthcare', 'GPS':'Technology',\n            'MMM': 'Construction', 'DELL': 'Technology', 'AMD':'Technology'}\n\n\n\n\npf.tears.create_round_trip_tear_sheet(positions, transactions, sector_mappings=sect_map)\n\n\n\n\n                      duration           pnl      returns      long\ncount                     1430   1430.000000  1430.000000      1430\nmean    9 days 16:40:56.154545     45.737238     0.003543  0.523077\nstd    22 days 02:16:41.165898   1616.537844     0.031288  0.499642\nmin            0 days 00:00:00 -30697.460000    -0.218045     False\n25%            0 days 23:59:59     -5.773144    -0.011450         0\n50%            2 days 23:59:59      0.871629     0.003885         1\n75%            5 days 23:59:59     40.438366     0.018126         1\nmax          286 days 00:00:00  17835.869482     0.204385      True\nPercent of round trips profitable = 57.2%\nMean return per winning round trip = 0.02181\nMean return per losing round trip = -0.02108\nA decision is made every 1.053 days.\n0.9495 trading decisions per day.\n19.94 trading decisions per month.\n\nProfitability (PnL / PnL total) per name:\nsymbol\nCOST    0.398964\nINTC    0.382659\nCERN    0.323077\nMMM     0.221479\nGPS     0.049385\nAMD    -0.064091\nDELL   -0.311473\nName: pnl, dtype: float64\n\nProfitability (PnL / PnL total) per name:\nsymbol\nConsumer Goods    0.398964\nHealthcare        0.323077\nConstruction      0.221479\nTechnology        0.056480\nName: pnl, dtype: float64\n\n\n\n\nmatplotlib.figure.Figure at 0x7f334aae1f28", 
            "title": "Round trip analysis"
        }, 
        {
            "location": "/round_trip_example/#round-trip-tearsheet", 
            "text": "When evaluating the performance of an investing strategy, it is helpful to quantify the frequency, duration, and profitability of its independent bets, or \"round trip\" trades. A round trip trade is started when a new long or short position is opened and is only completed when the number of shares in that position returns to or crosses zero.   The intent of the round trip tearsheet is to help differentiate strategies that profited off a few lucky trades from strategies that profited repeatedly from genuine alpha. Breaking down round trip profitability by traded name and sector can also help inform universe selection and identify exposure risks. For example, even if your equity curve looks robust, if only two securities in your universe of fifteen names contributed to overall profitability, you may have reason to question the logic of your strategy.  To identify round trips, pyfolio groups transactions by symbol and identifies the points at which each position amount leaves and returns to zero. Behind the scenes, transactions that cause position amounts to flip directly from long to short or short to long are divided into separate transactions so that distinct round trips can be identified. In calculating round trips, pyfolio will also append position closing transactions at the last timestamp in the positions data. This closing transaction will cause the PnL from any open positions to realized as completed round trips.  Note: The round trip method of quantifying performance is not applicable to every style of strategy.  For instance, simple rebalancing algorithms make very few, if any, round trip trades. The results of the round trip tearsheet will be less informative for any strategy that doesn't entirely exit positions.  %matplotlib inline\nimport gzip\nimport os\nimport pandas as pd\nimport pyfolio as pf  transactions = pd.read_csv(gzip.open('../tests/test_data/test_txn.csv.gz'),\n                    index_col=0, parse_dates=0)\npositions = pd.read_csv(gzip.open('../tests/test_data/test_pos.csv.gz'),\n                    index_col=0, parse_dates=0)\nreturns = pd.read_csv(gzip.open('../tests/test_data/test_returns.csv.gz'),\n                    index_col=0, parse_dates=0)  # Optional: Sector mappings may be passed in as a dict or pd.Series. If a mapping is\n# provided, PnL from symbols with mappings will be summed to display profitability by sector.\nsect_map = {'COST': 'Consumer Goods', 'INTC':'Technology', 'CERN':'Healthcare', 'GPS':'Technology',\n            'MMM': 'Construction', 'DELL': 'Technology', 'AMD':'Technology'}  pf.tears.create_round_trip_tear_sheet(positions, transactions, sector_mappings=sect_map)                        duration           pnl      returns      long\ncount                     1430   1430.000000  1430.000000      1430\nmean    9 days 16:40:56.154545     45.737238     0.003543  0.523077\nstd    22 days 02:16:41.165898   1616.537844     0.031288  0.499642\nmin            0 days 00:00:00 -30697.460000    -0.218045     False\n25%            0 days 23:59:59     -5.773144    -0.011450         0\n50%            2 days 23:59:59      0.871629     0.003885         1\n75%            5 days 23:59:59     40.438366     0.018126         1\nmax          286 days 00:00:00  17835.869482     0.204385      True\nPercent of round trips profitable = 57.2%\nMean return per winning round trip = 0.02181\nMean return per losing round trip = -0.02108\nA decision is made every 1.053 days.\n0.9495 trading decisions per day.\n19.94 trading decisions per month.\n\nProfitability (PnL / PnL total) per name:\nsymbol\nCOST    0.398964\nINTC    0.382659\nCERN    0.323077\nMMM     0.221479\nGPS     0.049385\nAMD    -0.064091\nDELL   -0.311473\nName: pnl, dtype: float64\n\nProfitability (PnL / PnL total) per name:\nsymbol\nConsumer Goods    0.398964\nHealthcare        0.323077\nConstruction      0.221479\nTechnology        0.056480\nName: pnl, dtype: float64 matplotlib.figure.Figure at 0x7f334aae1f28", 
            "title": "Round Trip Tearsheet"
        }, 
        {
            "location": "/slippage_example/", 
            "text": "Slippage Analysis\n\n\nWhen evaluating a strategy using backtest results, we often want to know how sensitive it's performance is to implementation shortfall or slippage. pyfolio's transactions tear sheet can create \"slippage sweep\" plots that display strategy performance under various slippage assumptions. \n\n\nAdditional per-dollar slippage can be applied to returns before running a tear sheet by providing \ncreate_full_tearsheet\n with the a level of slippage in basis points (1% == 100 basis points) as the \nslippage\n keyword argument. The slippage plots in the transactions tear sheet will display returns with slippage added to the \nunadjusted\n returns. \n\n\nFor example, if you run a backtest with no transaction costs and call \ncreate_full_tearsheet(returns, positions, transactions, slippage=5)\n, 5 bps of slippage will be applied to \nreturns\n before all plots and figures, with the exception of the slippage sweep plots, are generated.\n\n\nIt is important to emphasize that the slippage plots will display performance under \nadditional\n slippage. If the passed performance data already has slippage applied, the 5 bps slippage equity curve will represent performance under 5 bps of slippage in addition to the already simulated slippage penalty. If slippage is already applied to the performance results, pass \nslippage=0\n to the \ncreate_full_tearsheet\n to trigger the creation of the additional slippage sweep plots without applying any additional slippage to the returns time series used throughout the rest of the tear sheet.\n\n\n%matplotlib inline\nimport pyfolio as pf\nimport gzip\nimport pandas as pd\n\n\n\n\ntransactions = pd.read_csv(gzip.open('../tests/test_data/test_txn.csv.gz'),\n                    index_col=0, parse_dates=0)\npositions = pd.read_csv(gzip.open('../tests/test_data/test_pos.csv.gz'),\n                    index_col=0, parse_dates=0)\nreturns = pd.read_csv(gzip.open('../tests/test_data/test_returns.csv.gz'),\n                    index_col=0, parse_dates=0, header=None)[1]\ngross_lev = pd.read_csv(gzip.open('../tests/test_data/test_gross_lev.csv.gz'),\n                    index_col=0, parse_dates=0, header=None)[1]\nreturns.index = returns.index.tz_localize(\nUTC\n)\npositions.index = positions.index.tz_localize(\nUTC\n)\ntransactions.index = transactions.index.tz_localize(\nUTC\n)\ngross_lev.index = gross_lev.index.tz_localize(\nUTC\n)\n\n\n\n\npf.create_full_tear_sheet(returns, positions, transactions, gross_lev=gross_lev, slippage=0, live_start_date, round)\n\n\n\n\nEntire data start date: 2004-01-09\nEntire data end date: 2009-12-31\n\n\nBacktest Months: 71\n                   Backtest\nannual_return          0.12\nannual_volatility      0.26\nsharpe_ratio           0.45\ncalmar_ratio           0.20\nstability              0.00\nmax_drawdown          -0.60\nomega_ratio            1.09\nsortino_ratio          0.66\nskewness               0.14\nkurtosis               5.88\ninformation_ratio      0.03\nalpha                  0.08\nbeta                   0.83\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              60.39 2007-11-06  2009-03-09           NaT      NaN\n1              24.10 2005-07-28  2006-09-07    2007-05-22      474\n4              11.89 2004-06-25  2004-08-12    2004-11-05       96\n2              10.87 2004-11-15  2005-04-18    2005-07-14      174\n3               9.51 2007-07-16  2007-08-06    2007-09-13       44\n\n\n2-sigma returns daily    -0.033\n2-sigma returns weekly   -0.068\ndtype: float64\n\n\n\n\n\nStress Events\n                             mean    min    max\nLehmann                    -0.003 -0.047  0.041\nAug07                       0.003 -0.030  0.029\nMar08                      -0.004 -0.033  0.034\nSept08                     -0.007 -0.044  0.041\n2009Q1                     -0.004 -0.050  0.034\n2009Q2                      0.007 -0.040  0.061\nLow Volatility Bull Market  0.000 -0.061  0.064\nGFC Crash                  -0.001 -0.118  0.101\nRecovery                    0.004 -0.040  0.060\n\n\n\n\n\nTop 10 long positions of all time (and max%)\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\nTop 10 short positions of all time (and max%)\n['AMD' 'DELL' 'CERN' 'MMM' 'GPS' 'INTC' 'COST']\n[-0.301 -0.266 -0.255 -0.226 -0.201 -0.185 -0.164]\n\n\nTop 10 positions of all time (and max%)\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\nAll positions ever held\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]", 
            "title": "Slippage analysis"
        }, 
        {
            "location": "/slippage_example/#slippage-analysis", 
            "text": "When evaluating a strategy using backtest results, we often want to know how sensitive it's performance is to implementation shortfall or slippage. pyfolio's transactions tear sheet can create \"slippage sweep\" plots that display strategy performance under various slippage assumptions.   Additional per-dollar slippage can be applied to returns before running a tear sheet by providing  create_full_tearsheet  with the a level of slippage in basis points (1% == 100 basis points) as the  slippage  keyword argument. The slippage plots in the transactions tear sheet will display returns with slippage added to the  unadjusted  returns.   For example, if you run a backtest with no transaction costs and call  create_full_tearsheet(returns, positions, transactions, slippage=5) , 5 bps of slippage will be applied to  returns  before all plots and figures, with the exception of the slippage sweep plots, are generated.  It is important to emphasize that the slippage plots will display performance under  additional  slippage. If the passed performance data already has slippage applied, the 5 bps slippage equity curve will represent performance under 5 bps of slippage in addition to the already simulated slippage penalty. If slippage is already applied to the performance results, pass  slippage=0  to the  create_full_tearsheet  to trigger the creation of the additional slippage sweep plots without applying any additional slippage to the returns time series used throughout the rest of the tear sheet.  %matplotlib inline\nimport pyfolio as pf\nimport gzip\nimport pandas as pd  transactions = pd.read_csv(gzip.open('../tests/test_data/test_txn.csv.gz'),\n                    index_col=0, parse_dates=0)\npositions = pd.read_csv(gzip.open('../tests/test_data/test_pos.csv.gz'),\n                    index_col=0, parse_dates=0)\nreturns = pd.read_csv(gzip.open('../tests/test_data/test_returns.csv.gz'),\n                    index_col=0, parse_dates=0, header=None)[1]\ngross_lev = pd.read_csv(gzip.open('../tests/test_data/test_gross_lev.csv.gz'),\n                    index_col=0, parse_dates=0, header=None)[1]\nreturns.index = returns.index.tz_localize( UTC )\npositions.index = positions.index.tz_localize( UTC )\ntransactions.index = transactions.index.tz_localize( UTC )\ngross_lev.index = gross_lev.index.tz_localize( UTC )  pf.create_full_tear_sheet(returns, positions, transactions, gross_lev=gross_lev, slippage=0, live_start_date, round)  Entire data start date: 2004-01-09\nEntire data end date: 2009-12-31\n\n\nBacktest Months: 71\n                   Backtest\nannual_return          0.12\nannual_volatility      0.26\nsharpe_ratio           0.45\ncalmar_ratio           0.20\nstability              0.00\nmax_drawdown          -0.60\nomega_ratio            1.09\nsortino_ratio          0.66\nskewness               0.14\nkurtosis               5.88\ninformation_ratio      0.03\nalpha                  0.08\nbeta                   0.83\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              60.39 2007-11-06  2009-03-09           NaT      NaN\n1              24.10 2005-07-28  2006-09-07    2007-05-22      474\n4              11.89 2004-06-25  2004-08-12    2004-11-05       96\n2              10.87 2004-11-15  2005-04-18    2005-07-14      174\n3               9.51 2007-07-16  2007-08-06    2007-09-13       44\n\n\n2-sigma returns daily    -0.033\n2-sigma returns weekly   -0.068\ndtype: float64   Stress Events\n                             mean    min    max\nLehmann                    -0.003 -0.047  0.041\nAug07                       0.003 -0.030  0.029\nMar08                      -0.004 -0.033  0.034\nSept08                     -0.007 -0.044  0.041\n2009Q1                     -0.004 -0.050  0.034\n2009Q2                      0.007 -0.040  0.061\nLow Volatility Bull Market  0.000 -0.061  0.064\nGFC Crash                  -0.001 -0.118  0.101\nRecovery                    0.004 -0.040  0.060   Top 10 long positions of all time (and max%)\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\nTop 10 short positions of all time (and max%)\n['AMD' 'DELL' 'CERN' 'MMM' 'GPS' 'INTC' 'COST']\n[-0.301 -0.266 -0.255 -0.226 -0.201 -0.185 -0.164]\n\n\nTop 10 positions of all time (and max%)\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]\n\n\nAll positions ever held\n['COST' 'DELL' 'CERN' 'MMM' 'INTC' 'AMD' 'GPS']\n[ 0.9    0.857  0.835  0.821  0.786  0.758  0.622]", 
            "title": "Slippage Analysis"
        }
    ]
}